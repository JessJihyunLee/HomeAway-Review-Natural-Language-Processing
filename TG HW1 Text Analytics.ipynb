{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\teeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "import random\n",
    "from patsy import dmatrices\n",
    "#nltk.download()\n",
    "from string import punctuation\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\teeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set (stopwords.words( 'english' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/UTA/Fall term/Text analytics/HW1/Train_rev1.csv\")\n",
    "df = data.sample(2500, random_state = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk =[]\n",
    "tk_fil =[]\n",
    "for sen in df['FullDescription']:\n",
    "    #make sentence lowercase\n",
    "    sen = sen.lower()\n",
    "    #remove punctuation\n",
    "    for p in punctuation:\n",
    "        sen=sen.replace(p,'') \n",
    "    #tokenize to words\n",
    "    txt = word_tokenize(sen)\n",
    "    #tag pos\n",
    "    tk += nltk.pos_tag(txt)\n",
    "    #remove stopwords\n",
    "    txt_fil = [word for word in txt if word not in stop_words]\n",
    "    tk_fil += nltk.pos_tag(txt_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 156816), ('JJ', 69903), ('IN', 65426), ('NNS', 50777), ('DT', 50574)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tk_lst = Counter(tag for (word, tag) in tk)\n",
    "print (tk_lst.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 151968), ('JJ', 71969), ('NNS', 50279), ('VBG', 25536), ('VBP', 16812)]\n"
     ]
    }
   ],
   "source": [
    "tk_lst_fil = Counter(tag for (word, tag) in tk_fil)\n",
    "print (tk_lst_fil.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2 . Does this data support Zipfâ€™s law? Plot the most common 100 words in the data against the theoretical prediction of the law. For this question, do not remove stopwords. Also do not perform stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_zf = []\n",
    "for sen in df['FullDescription']:\n",
    "    #make sentence lowercase\n",
    "    sen = sen.lower()\n",
    "    #remove punctuation\n",
    "    for p in punctuation:\n",
    "        sen=sen.replace(p,'') \n",
    "    #tokenize to words\n",
    "    txt_zf += word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_zf = Counter(txt_zf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = tk_zf.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = []\n",
    "for (word, freq) in topwords:\n",
    "    frequency.append(freq)\n",
    "\n",
    "frequency = pd.Series(frequency)\n",
    "\n",
    "# get the observational rank# get th \n",
    "word_ranking = frequency.rank(method = 'min',ascending =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_rank = pd.DataFrame(dict(Frequency = frequency,Rank = word_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_rank['logrank']= np.log(df_freq_rank.Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2500*df_freq_rank['Frequency'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_rank['logfreq']= np.log((df_freq_rank.Frequency)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                logrank   R-squared:                       0.988\n",
      "Model:                            OLS   Adj. R-squared:                  0.988\n",
      "Method:                 Least Squares   F-statistic:                     8399.\n",
      "Date:                Tue, 11 Sep 2018   Prob (F-statistic):           8.78e-97\n",
      "Time:                        21:49:58   Log-Likelihood:                 89.216\n",
      "No. Observations:                 100   AIC:                            -174.4\n",
      "Df Residuals:                      98   BIC:                            -169.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -7.8071      0.125    -62.322      0.000      -8.056      -7.559\n",
      "logfreq       -1.0864      0.012    -91.647      0.000      -1.110      -1.063\n",
      "==============================================================================\n",
      "Omnibus:                      105.527   Durbin-Watson:                   0.474\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2244.342\n",
      "Skew:                          -3.235   Prob(JB):                         0.00\n",
      "Kurtosis:                      25.288   Cond. No.                         133.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "y, X = dmatrices('logrank ~ logfreq', data=df_freq_rank, return_type='dataframe')\n",
    "model = sm.OLS(y, X)       # Set up the model\n",
    "result = model.fit()       # Fit model (find the intercept and slopes)\n",
    "print (result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. If we remove stopwords and lemmatize the data, what are the 10 most common words? What are their frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_lem =[]\n",
    "for sen in df['FullDescription']:\n",
    "    #make sentence lowercase\n",
    "    sen = sen.lower()\n",
    "    #remove punctuation\n",
    "    for p in punctuation:\n",
    "        sen=sen.replace(p,'') \n",
    "    txt_lem += word_tokenize(sen)\n",
    "#remove stopwords\n",
    "tk_lem = [word for word in txt_lem if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "tk_lem_proc = [WordNetLemmatizer().lemmatize(t, pos = 'v') for t in tk_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('work', 5283), ('experience', 5110), ('team', 3058), ('role', 2952), ('business', 2747), ('service', 2444), ('skills', 2353), ('sales', 2181), ('within', 2181), ('client', 2006)]\n"
     ]
    }
   ],
   "source": [
    "tk_lem_lst = Counter(tk_lem_proc)\n",
    "print (tk_lem_lst.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_pos(word):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "    \n",
    "tk_lem_proc = [WordNetLemmatizer().lemmatize(t, get_pos(t)) for t in tk_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('work', 5283), ('experience', 5110), ('role', 3160), ('team', 3058), ('client', 2994), ('business', 2864), ('service', 2444), ('skill', 2420), ('sale', 2209), ('within', 2181)]\n"
     ]
    }
   ],
   "source": [
    "tk_lem_lst = Counter(tk_lem_proc)\n",
    "print (tk_lem_lst.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B (predict salary from job description; the idea here is to test the predictive power of text and compare it with that of numeric data)\n",
    "In this section, you will create classification models to predict high (75th percentile and above) or low (below 75th percentile) salary from the text contained in the job descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B1. Ignore the job descriptions, and train a model to predict high/low salary from all the numeric columns, e.g., part time/full time, contract vs. others, type of job (a lot of dummy variables), location (instead of using a huge number of dummy variables, you can use a list of cities in England with highest cost of living, and create a 0/1 variable which is 1 if the job is in one of those cities, else 0). Use the NaÃ¯ve Bayes classifier. What is the accuracy of your model?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:,3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teeru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df1['HighSalary'] = 0.0\n",
    "mask = df1['SalaryNormalized'] > df1['SalaryNormalized'].quantile(.75)\n",
    "df1['HighSalary'][mask] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlivingcost = ['oxford','uk','the city','greater london','central london','london','south east london','winchester', 'cambridge', 'chichester', 'brighon and hove', 'bath', 'southhampton','salisbury','canterbury','st albans', 'bristol','lichfield','truro','norwich','chelmsford','exeter','york','leicester','gloucester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teeru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def lowerize(x):\n",
    "  return x.lower()\n",
    "\n",
    "df1['HighLocation'] = 0.0\n",
    "df1['HighLocation'][df1['LocationNormalized'].apply(lowerize).isin(highlivingcost)] = 1.0\n",
    "df1 = df1.drop(columns=['LocationRaw','SalaryRaw','LocationNormalized','Company', 'SalaryNormalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['ContractType', 'ContractTime', 'Category']\n",
    "df_dummies = pd.get_dummies(df1[categorical_columns],\n",
    "                            prefix=categorical_columns,\n",
    "                            columns=categorical_columns)\n",
    "dummy_column_names = df_dummies.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df1, df_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'HighSalary ~ 0 + {}'.format(' + '.join(['Q(\"{}\")'.format(x) for x in dummy_column_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "Y, X = dmatrices(formula, df2, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y['HighSalary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "model = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(99)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Baseline accuracy\n",
    "print ((len(y_test) - y_test.sum()) /len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "prediction_train = model.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, prediction_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "model = naive_bayes.BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "prediction_train = model.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, prediction_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teeru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df3['HighSalary'] = 0.0\n",
    "mask = df3['SalaryNormalized'] > df3['SalaryNormalized'].quantile(.75)\n",
    "df3['HighSalary'][mask] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[['FullDescription','HighSalary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus\n",
    "bow = []\n",
    "for sen in df4['FullDescription']:\n",
    "    sen = sen.lower()\n",
    "    for p in punctuation:\n",
    "        sen = sen.replace(p,\"\")\n",
    "    bow.append(sen)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "count = vectorizer.fit_transform(bow).todense() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = vectorizer.get_feature_names()\n",
    "df5 = pd.SparseDataFrame(count, columns=ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.reset_index()\n",
    "df4 = df4[['HighSalary','FullDescription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.join(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df6.drop(columns = 'FullDescription')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'HighSalary ~ 0 + {}'.format(' + '.join(['Q(\"{}\")'.format(x) for x in ft]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text = df6['HighSalary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(df5, y_text, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_text, y_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "prediction_train_text = model.predict(X_test_text)\n",
    "print (metrics.accuracy_score(y_test_text, prediction_train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df2.drop(columns = ['ContractType','ContractTime','Category', 'HighSalary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df7.reset_index().drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df6.join(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = df8.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'HighSalary ~ 0 + {}'.format(' + '.join(['Q(\"{}\")'.format(x) for x in var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hybrid = df8['HighSalary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df8.drop(columns = 'HighSalary')\n",
    "X_train_hybrid, X_test_hybrid, y_train_hybrid, y_test_hybrid = train_test_split(df9, y_hybrid, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_hybrid, y_train_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026666666666666\n"
     ]
    }
   ],
   "source": [
    "prediction_train_hybrid = model.predict(X_test_hybrid)\n",
    "print (metrics.accuracy_score(y_test_hybrid, prediction_train_hybrid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
